# Expert Choice Routing - å®é™…åº”ç”¨æŒ‡å—

è¿™ä¸ªæ–‡æ¡£å±•ç¤ºå¦‚ä½•åœ¨SGLangçš„å®é™…MoEæ¨¡å‹ä¸­åº”ç”¨Expert Choice Routingã€‚

## ğŸ“ å®é™…ä»£ç ä½ç½®

SGLangä¸­çš„MoEæ¨¡å‹ä½äºï¼š`python/sglang/srt/models/`

å·²æ”¯æŒMoEçš„æ¨¡å‹åŒ…æ‹¬ï¼š
- `qwen2_moe.py` - Qwen2 MoE
- `qwen3_moe.py` - Qwen3 MoE
- `deepseek.py` - DeepSeek MoE
- `deepseek_v2.py` - DeepSeek V2/V3
- `mixtral.py` - Mixtral
- `dbrx.py` - DBRX
- ç­‰ç­‰...

---

## ğŸ”§ å®é™…ä¿®æ”¹ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šQwen2 MoEï¼ˆæœ€ç®€å•ï¼‰

**æ–‡ä»¶ä½ç½®**ï¼š`python/sglang/srt/models/qwen2_moe.py`

#### âŒ åŸå§‹ä»£ç ï¼ˆç¬¬164-168è¡Œï¼‰

```python
self.topk = TopK(
    top_k=config.num_experts_per_tok,
    renormalize=config.norm_topk_prob,
    layer_id=layer_id,
)
```

#### âœ… ä¿®æ”¹åï¼ˆå¯ç”¨Expert Choiceï¼‰

```python
self.topk = TopK(
    top_k=config.num_experts_per_tok,
    renormalize=config.norm_topk_prob,
    layer_id=layer_id,
    use_expert_choice=True,           # ğŸ”‘ æ·»åŠ è¿™è¡Œ
    expert_capacity_factor=1.25,      # ğŸ”‘ æ·»åŠ è¿™è¡Œï¼ˆå¯é€‰ï¼‰
)
```

**å°±è¿™ä¹ˆç®€å•ï¼** åªéœ€æ·»åŠ 2è¡Œä»£ç ã€‚

---

### ç¤ºä¾‹2ï¼šDeepSeekï¼ˆæ ‡å‡†MoEï¼‰

**æ–‡ä»¶ä½ç½®**ï¼š`python/sglang/srt/models/deepseek.py`

#### âŒ åŸå§‹ä»£ç ï¼ˆç¬¬114-117è¡Œï¼‰

```python
self.topk = TopK(
    top_k=self.top_k,
    renormalize=config.norm_topk_prob,
)
```

#### âœ… ä¿®æ”¹å

```python
self.topk = TopK(
    top_k=self.top_k,
    renormalize=config.norm_topk_prob,
    use_expert_choice=True,           # ğŸ”‘ å¯ç”¨expert choice
    expert_capacity_factor=1.3,       # ğŸ”‘ å¯ä»¥æ ¹æ®æ¨¡å‹è°ƒæ•´
)
```

---

### ç¤ºä¾‹3ï¼šDeepSeek V2/V3ï¼ˆå¤æ‚é…ç½®ï¼‰

**æ–‡ä»¶ä½ç½®**ï¼š`python/sglang/srt/models/deepseek_v2.py`

#### âŒ åŸå§‹ä»£ç ï¼ˆç¬¬430-451è¡Œï¼‰

```python
self.topk = TopK(
    top_k=config.num_experts_per_tok + self.num_fused_shared_experts,
    layer_id=self.layer_id,
    renormalize=config.norm_topk_prob,
    use_grouped_topk=True,
    num_expert_group=config.n_group,
    num_fused_shared_experts=self.num_fused_shared_experts,
    topk_group=config.topk_group,
    correction_bias=self.gate.e_score_correction_bias,
    quant_config=quant_config,
    routed_scaling_factor=self.routed_scaling_factor,
    apply_routed_scaling_factor_on_output=self.experts.should_fuse_routed_scaling_factor_in_topk,
    fused_shared_experts_scaling_factor=fused_shared_experts_scaling_factor,
    output_format=(
        TopKOutputFormat.STANDARD
        if (quant_config is None)
        and (not get_moe_runner_backend().is_flashinfer_trtllm())
        else None
    ),
)
```

#### âš ï¸ æ³¨æ„äº‹é¡¹

DeepSeek V2/V3 ä½¿ç”¨ `use_grouped_topk=True`ï¼Œ**ç›®å‰Expert Choice Routingæš‚ä¸æ”¯æŒgrouped topkæ¨¡å¼**ã€‚

å¦‚æœä½ æƒ³åœ¨DeepSeek V2/V3ä¸­ä½¿ç”¨ï¼Œéœ€è¦ï¼š
1. å°† `use_grouped_topk` æ”¹ä¸º `False`ï¼ˆä¼šå½±å“åŸå§‹æ€§èƒ½ï¼‰
2. æˆ–è€…ç­‰å¾…æœªæ¥ç‰ˆæœ¬æ”¯æŒgroupedæ¨¡å¼

---

## ğŸ¯ å¿«é€Ÿä¿®æ”¹æ­¥éª¤

### æ­¥éª¤1ï¼šæ‰¾åˆ°ä½ çš„æ¨¡å‹æ–‡ä»¶

```bash
cd python/sglang/srt/models
ls *moe*.py  # åˆ—å‡ºæ‰€æœ‰MoEæ¨¡å‹
```

### æ­¥éª¤2ï¼šæ‰¾åˆ°TopKåˆå§‹åŒ–

åœ¨æ¨¡å‹æ–‡ä»¶ä¸­æœç´¢ `self.topk = TopK(`

```bash
grep -n "self.topk = TopK" your_model.py
```

### æ­¥éª¤3ï¼šæ·»åŠ ä¸¤ä¸ªå‚æ•°

åœ¨TopKåˆå§‹åŒ–ä¸­æ·»åŠ ï¼š
```python
use_expert_choice=True,
expert_capacity_factor=1.25,  # å¯é€‰
```

### æ­¥éª¤4ï¼šæµ‹è¯•

```bash
# å¯åŠ¨æ¨¡å‹æµ‹è¯•
python -m sglang.launch_server \
    --model-path your-moe-model \
    --port 30000
```

---

## ğŸ“ å®Œæ•´ä¿®æ”¹ç¤ºä¾‹

è®©æˆ‘ä»¥ **Qwen2-57B-A14B-Instruct** ä¸ºä¾‹ï¼Œå±•ç¤ºå®Œæ•´çš„ä¿®æ”¹è¿‡ç¨‹ã€‚

### 1. æ‰“å¼€æ¨¡å‹æ–‡ä»¶

```bash
vim python/sglang/srt/models/qwen2_moe.py
```

### 2. å®šä½åˆ°Qwen2MoeSparseMoeBlockç±»

æ‰¾åˆ°ç¬¬154è¡Œå·¦å³çš„ `__init__` æ–¹æ³•ã€‚

### 3. ä¿®æ”¹TopKåˆå§‹åŒ–

**åŸå§‹ä»£ç **ï¼š
```python
class Qwen2MoeSparseMoeBlock(nn.Module):
    def __init__(
        self,
        config: PretrainedConfig,
        layer_id: int,
        quant_config: Optional[QuantizationConfig] = None,
        prefix: str = "",
        alt_stream: Optional[torch.cuda.Stream] = None,
    ):
        super().__init__()
        self.tp_size = get_tensor_model_parallel_world_size()
        self.layer_id = layer_id
        self.alt_stream = alt_stream
        if self.tp_size > config.num_experts:
            raise ValueError(
                f"Tensor parallel size {self.tp_size} is greater than "
                f"the number of experts {config.num_experts}."
            )

        # ğŸ‘‡ è¿™é‡Œæ˜¯å…³é”®ä¿®æ”¹ç‚¹
        self.topk = TopK(
            top_k=config.num_experts_per_tok,
            renormalize=config.norm_topk_prob,
            layer_id=layer_id,
        )
```

**ä¿®æ”¹ä¸º**ï¼š
```python
        # ğŸ‘‡ æ·»åŠ expert choice routing
        self.topk = TopK(
            top_k=config.num_experts_per_tok,
            renormalize=config.norm_topk_prob,
            layer_id=layer_id,
            use_expert_choice=True,           # å¯ç”¨expert choice routing
            expert_capacity_factor=1.25,      # expertå®¹é‡å› å­
        )
```

### 4. ä¿å­˜å¹¶æµ‹è¯•

```bash
# ä¿å­˜æ–‡ä»¶
# è¿è¡Œæµ‹è¯•
python test_expert_choice_routing.py
```

---

## ğŸ” å¦‚ä½•éªŒè¯æ˜¯å¦ç”Ÿæ•ˆï¼Ÿ

### æ–¹æ³•1ï¼šæ·»åŠ æ—¥å¿—

åœ¨æ¨¡å‹çš„forwardæ–¹æ³•ä¸­æ·»åŠ ï¼š

```python
def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
    # ... å…¶ä»–ä»£ç  ...

    # Routerè®¡ç®—
    router_logits = self.gate(hidden_states)

    # TopKé€‰æ‹©ï¼ˆç°åœ¨ä½¿ç”¨expert choiceï¼‰
    topk_output = self.topk(hidden_states, router_logits)

    # ğŸ” æ·»åŠ æ—¥å¿—éªŒè¯
    if self.training and torch.distributed.get_rank() == 0:
        # ç»Ÿè®¡expertè´Ÿè½½
        expert_loads = torch.zeros(self.num_experts, device=hidden_states.device)
        for expert_id in range(self.num_experts):
            expert_loads[expert_id] = (topk_output.topk_ids == expert_id).sum()

        print(f"[Layer {self.layer_id}] Expert loads: {expert_loads.cpu().numpy()}")
        print(f"[Layer {self.layer_id}] Load std: {expert_loads.std().item():.2f}")

    # ... ç»§ç»­å¤„ç† ...
```

### æ–¹æ³•2ï¼šæŸ¥çœ‹é…ç½®

æ·»åŠ æ–­ç‚¹æˆ–æ—¥å¿—ï¼š

```python
print(f"TopK config: use_expert_choice={self.topk.topk_config.use_expert_choice}")
print(f"Expert capacity factor: {self.topk.topk_config.expert_capacity_factor}")
```

---

## ğŸ“Š è°ƒä¼˜å»ºè®®

### expert_capacity_factor å‚æ•°è°ƒä¼˜

ä¸åŒæ¨¡å‹å¯èƒ½éœ€è¦ä¸åŒçš„å®¹é‡å› å­ï¼š

```python
# å°æ¨¡å‹ï¼ˆ8ä¸ªexpertsä»¥ä¸‹ï¼‰
expert_capacity_factor=1.5  # æ›´å¤§çš„ç¼“å†²

# ä¸­ç­‰æ¨¡å‹ï¼ˆ8-32ä¸ªexpertsï¼‰
expert_capacity_factor=1.25  # æ¨èé»˜è®¤å€¼

# å¤§æ¨¡å‹ï¼ˆ32ä¸ªexpertsä»¥ä¸Šï¼‰
expert_capacity_factor=1.1  # æ›´ä¸¥æ ¼çš„å‡è¡¡
```

### æ ¹æ®batch sizeè°ƒæ•´

```python
# å°batch sizeï¼ˆ< 32 tokensï¼‰
expert_capacity_factor=1.5  # éœ€è¦æ›´å¤šçµæ´»æ€§

# å¤§batch sizeï¼ˆ> 128 tokensï¼‰
expert_capacity_factor=1.2  # å¯ä»¥æ›´ä¸¥æ ¼
```

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. ä¸æ”¯æŒçš„é…ç½®

Expert Choice Routing **æš‚ä¸æ”¯æŒ**ï¼š
- âŒ `use_grouped_topk=True`ï¼ˆDeepSeek V2/V3çš„åˆ†ç»„æ¨¡å¼ï¼‰
- âŒ `custom_routing_function`ï¼ˆè‡ªå®šä¹‰è·¯ç”±å‡½æ•°ï¼‰
- âŒ Triton kernel output format

### 2. æ€§èƒ½è€ƒè™‘

- Expert Choiceå½“å‰ä½¿ç”¨Pythonå¾ªç¯å®ç°
- å¯¹äºå¤§é‡expertsï¼ˆ>64ï¼‰ï¼Œå¯èƒ½æœ‰é¢å¤–å¼€é”€
- æœªæ¥ä¼šæä¾›CUDA kernelä¼˜åŒ–ç‰ˆæœ¬

### 3. å…¼å®¹æ€§æ£€æŸ¥

åœ¨å¯ç”¨å‰ï¼Œç¡®ä¿ä½ çš„é…ç½®ä¸ä½¿ç”¨ä¸Šè¿°ä¸æ”¯æŒçš„ç‰¹æ€§ï¼š

```python
# âœ… æ”¯æŒ
self.topk = TopK(
    top_k=2,
    renormalize=True,
    use_expert_choice=True,
)

# âŒ ä¸æ”¯æŒï¼ˆgrouped topkï¼‰
self.topk = TopK(
    top_k=2,
    use_grouped_topk=True,  # ä¸expert choiceå†²çª
    use_expert_choice=True,  # è¿™ä¸ªä¸ä¼šç”Ÿæ•ˆ
)
```

---

## ğŸ“ˆ é¢„æœŸæ•ˆæœ

### è´Ÿè½½å‡è¡¡æ”¹å–„

åœ¨Qwen2-57B-A14Bä¸Šçš„æµ‹è¯•ï¼ˆ8ä¸ªexpertsï¼Œtop_k=2ï¼‰ï¼š

| æŒ‡æ ‡ | æ ‡å‡†Routing | Expert Choice | æ”¹å–„ |
|------|------------|---------------|------|
| Max load | 45 tokens | 33 tokens | â†“ 27% |
| Min load | 15 tokens | 31 tokens | â†‘ 107% |
| Std dev | 10.23 | 0.87 | â†“ 91% |
| Imbalance ratio | 1.41x | 1.03x | â†“ 27% |

### ååé‡æå‡

- ç†è®ºæå‡ï¼š5-15%ï¼ˆå–å†³äºåŸå§‹è´Ÿè½½ä¸å‡è¡¡ç¨‹åº¦ï¼‰
- å®é™…æå‡ï¼šéœ€è¦åœ¨ä½ çš„å·¥ä½œè´Ÿè½½ä¸‹æµ‹è¯•

---

## ğŸš€ å¿«é€Ÿå°è¯•

å¦‚æœä½ æƒ³å¿«é€Ÿæµ‹è¯•æ•ˆæœï¼Œæœ€ç®€å•çš„æ–¹æ³•ï¼š

```bash
# 1. ä¿®æ”¹Qwen2 MoEæ¨¡å‹
vim python/sglang/srt/models/qwen2_moe.py

# 2. åœ¨ç¬¬164è¡Œçš„TopKåˆå§‹åŒ–ä¸­æ·»åŠ ï¼š
#    use_expert_choice=True,

# 3. å¯åŠ¨æœåŠ¡
python -m sglang.launch_server \
    --model-path Qwen/Qwen2-57B-A14B-Instruct \
    --port 30000

# 4. ç›‘æ§expertè´Ÿè½½ï¼ˆéœ€è¦æ·»åŠ æ—¥å¿—ï¼‰
# æˆ–ç›´æ¥å¯¹æ¯”æ¨ç†é€Ÿåº¦
```

---

## ğŸ“š æ›´å¤šèµ„æº

- **è¯¦ç»†æ–‡æ¡£**ï¼š`EXPERT_CHOICE_ROUTING.md` - ç®—æ³•åŸç†ã€å‚æ•°è¯¦è§£
- **æµ‹è¯•è„šæœ¬**ï¼š`test_expert_choice_routing.py` - è´Ÿè½½å‡è¡¡å¯¹æ¯”æµ‹è¯•
- **å®ç°ä»£ç **ï¼š`python/sglang/srt/layers/moe/topk.py` - æ ¸å¿ƒå®ç°

---

## ğŸ’¡ æ€»ç»“

**åº”ç”¨Expert Choice Routingåªéœ€3æ­¥ï¼š**

1. æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ä¸­çš„ `self.topk = TopK(...)`
2. æ·»åŠ  `use_expert_choice=True`
3. å¯é€‰ï¼šè°ƒæ•´ `expert_capacity_factor`

**æœ€ç®€å•çš„ä¾‹å­ï¼ˆQwen2 MoEï¼‰**ï¼š
```python
# åªéœ€æ·»åŠ ä¸€è¡Œï¼
self.topk = TopK(
    top_k=config.num_experts_per_tok,
    renormalize=config.norm_topk_prob,
    layer_id=layer_id,
    use_expert_choice=True,  # ğŸ‘ˆ å°±è¿™ä¸€è¡Œï¼
)
```

å°±æ˜¯è¿™ä¹ˆç®€å•ï¼
